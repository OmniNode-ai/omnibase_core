#!/usr/bin/env python3
"""
Simple Canary Node Testing Script.

Tests the business logic of all canary nodes with simplified initialization
to validate functionality without complex infrastructure dependencies.
"""

import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field

# ===== SIMPLIFIED MODELS =====


class ModelCanaryComputeInput(BaseModel):
    """Input model for Canary Compute operations."""

    operation_type: str = Field(..., description="Type of computation to perform")
    data: Dict[str, Any] = Field(
        default_factory=dict, description="Input data for computation"
    )
    correlation_id: str = Field(default="", description="Request correlation ID")


class ModelCanaryComputeOutput(BaseModel):
    """Output model for Canary Compute operations."""

    result: Dict[str, Any] = Field(
        default_factory=dict, description="Computation result"
    )
    success: bool = Field(default=True, description="Whether operation succeeded")
    error_message: str = Field(
        default="", description="Error message if operation failed"
    )
    execution_time_ms: int = Field(
        default=0, description="Execution time in milliseconds"
    )
    correlation_id: str = Field(default="", description="Request correlation ID")


class ModelCanaryEffectInput(BaseModel):
    """Input model for Canary Effect operations."""

    operation_type: str = Field(..., description="Type of effect operation to perform")
    target_system: Optional[str] = Field(
        default=None, description="Target system for effect operation"
    )
    parameters: Dict[str, Any] = Field(
        default_factory=dict, description="Operation-specific parameters"
    )
    correlation_id: Optional[str] = Field(
        default=None, description="Request correlation ID"
    )


class ModelCanaryEffectOutput(BaseModel):
    """Output model for Canary Effect operations."""

    operation_result: Dict[str, Any] = Field(
        default_factory=dict, description="Result data from the effect operation"
    )
    success: bool = Field(default=True, description="Whether operation succeeded")
    error_message: Optional[str] = Field(
        default=None, description="Error message if operation failed"
    )
    execution_time_ms: int = Field(
        default=0, description="Execution time in milliseconds"
    )
    correlation_id: Optional[str] = Field(
        default=None, description="Request correlation ID"
    )


class ModelCanaryOrchestratorInput(BaseModel):
    """Input model for Canary Orchestrator operations."""

    operation_type: str = Field(..., description="Type of orchestration operation")
    workflow_id: str = Field(..., description="Unique workflow identifier")
    correlation_id: Optional[str] = Field(
        default=None, description="Request correlation ID"
    )
    parameters: Dict[str, Any] = Field(
        default_factory=dict, description="Operation-specific parameters"
    )


class ModelCanaryOrchestratorOutput(BaseModel):
    """Output model for Canary Orchestrator operations."""

    status: str = Field(..., description="Operation status")
    workflow_result: Dict[str, Any] = Field(
        default_factory=dict, description="Workflow execution result"
    )
    error_message: Optional[str] = Field(
        default=None, description="Error message if operation failed"
    )
    execution_metrics: Dict[str, Any] = Field(
        default_factory=dict, description="Execution metrics and timing"
    )


class ModelCanaryReducerInput(BaseModel):
    """Input model for Canary Reducer operations."""

    adapter_results: List[Dict[str, Any]] = Field(
        ..., description="Results from infrastructure adapters to aggregate"
    )
    operation_type: Optional[str] = Field(
        default="aggregate", description="Type of aggregation operation to perform"
    )


class ModelCanaryReducerOutput(BaseModel):
    """Output model for Canary Reducer operations."""

    status: str = Field(..., description="Operation status")
    aggregated_result: Dict[str, Any] = Field(
        default_factory=dict,
        description="Aggregated results from infrastructure adapters",
    )
    error_message: Optional[str] = Field(
        default=None, description="Error message if operation failed"
    )


class ModelGroupGatewayInput(BaseModel):
    """Input model for Group Gateway operations."""

    operation_type: str = Field(..., description="Type of gateway operation to perform")
    target_tools: List[str] = Field(
        ..., description="List of target tools for message routing"
    )
    message_data: Dict[str, Any] = Field(..., description="Message payload data")
    correlation_id: Optional[str] = Field(
        default=None, description="Request correlation ID"
    )
    timeout_ms: Optional[int] = Field(
        default=10000, description="Request timeout in milliseconds"
    )
    cache_strategy: Optional[str] = Field(
        default="default", description="Caching strategy to use"
    )


class ModelGroupGatewayOutput(BaseModel):
    """Output model for Group Gateway operations."""

    status: str = Field(..., description="Operation status")
    aggregated_response: Dict[str, Any] = Field(
        default_factory=dict, description="Aggregated response from target tools"
    )
    error_message: Optional[str] = Field(
        default=None, description="Error message if operation failed"
    )
    cache_hit: Optional[bool] = Field(
        default=None, description="Whether response was served from cache"
    )
    routing_metrics: Dict[str, Any] = Field(
        default_factory=dict, description="Routing performance metrics"
    )


# ===== SIMPLIFIED NODES =====


class SimpleNodeCanaryCompute:
    """Simplified Canary Compute Node for testing business logic."""

    def __init__(self):
        self._processed_count = 0

    async def process_computation(
        self, input_data: ModelCanaryComputeInput
    ) -> ModelCanaryComputeOutput:
        """Process a computation request."""
        start_time = time.time()

        try:
            operation_type = input_data.operation_type
            data = input_data.data

            if operation_type == "addition":
                result = self._perform_addition(data)
            elif operation_type == "multiplication":
                result = self._perform_multiplication(data)
            elif operation_type == "aggregation":
                result = self._perform_aggregation(data)
            else:
                raise ValueError(f"Unsupported operation type: {operation_type}")

            self._processed_count += 1
            execution_time = int((time.time() - start_time) * 1000)

            return ModelCanaryComputeOutput(
                result=result,
                success=True,
                execution_time_ms=execution_time,
                correlation_id=input_data.correlation_id,
            )

        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ModelCanaryComputeOutput(
                result={},
                success=False,
                error_message=str(e),
                execution_time_ms=execution_time,
                correlation_id=input_data.correlation_id,
            )

    def _perform_addition(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Perform addition operation."""
        values = data.get("values", [])
        if not values:
            raise ValueError("No values provided for addition")

        result = sum(values)
        return {"operation": "addition", "values": values, "result": result}

    def _perform_multiplication(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Perform multiplication operation."""
        values = data.get("values", [])
        if not values:
            raise ValueError("No values provided for multiplication")

        result = 1
        for value in values:
            result *= value

        return {"operation": "multiplication", "values": values, "result": result}

    def _perform_aggregation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Perform data aggregation."""
        items = data.get("items", [])
        if not items:
            raise ValueError("No items provided for aggregation")

        total = len(items)
        numeric_values = [item for item in items if isinstance(item, (int, float))]
        avg = sum(numeric_values) / len(numeric_values) if numeric_values else 0

        return {
            "operation": "aggregation",
            "total_items": total,
            "numeric_items": len(numeric_values),
            "average": avg,
            "summary": f"Aggregated {total} items with average {avg:.2f}",
        }


class SimpleNodeCanaryEffect:
    """Simplified Canary Effect Node for testing business logic."""

    def __init__(self):
        self._operations_count = 0
        self._mock_data = {}

    async def perform_effect(
        self, input_data: ModelCanaryEffectInput
    ) -> ModelCanaryEffectOutput:
        """Perform an effect operation."""
        start_time = time.time()

        try:
            operation_type = input_data.operation_type

            if operation_type == "health_check":
                result = await self._simulate_health_check()
            elif operation_type == "external_api_call":
                result = await self._simulate_api_call(input_data.parameters)
            elif operation_type == "file_system_operation":
                result = await self._simulate_file_operation(input_data.parameters)
            elif operation_type == "database_operation":
                result = await self._simulate_database_operation(input_data.parameters)
            elif operation_type == "message_queue_operation":
                result = await self._simulate_queue_operation(input_data.parameters)
            else:
                raise ValueError(f"Unsupported operation type: {operation_type}")

            self._operations_count += 1
            execution_time = int((time.time() - start_time) * 1000)

            return ModelCanaryEffectOutput(
                operation_result=result,
                success=True,
                execution_time_ms=execution_time,
                correlation_id=input_data.correlation_id,
            )

        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ModelCanaryEffectOutput(
                operation_result={},
                success=False,
                error_message=str(e),
                execution_time_ms=execution_time,
                correlation_id=input_data.correlation_id,
            )

    async def _simulate_health_check(self) -> Dict[str, Any]:
        """Simulate a health check operation."""
        await asyncio.sleep(0.1)  # Simulate network delay
        return {
            "operation": "health_check",
            "status": "healthy",
            "checks": {
                "database": "healthy",
                "cache": "healthy",
                "external_apis": "healthy",
            },
            "timestamp": time.time(),
        }

    async def _simulate_api_call(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate an external API call."""
        endpoint = parameters.get("endpoint", "/default")
        method = parameters.get("method", "GET")

        await asyncio.sleep(0.2)  # Simulate API call delay

        return {
            "operation": "api_call",
            "endpoint": endpoint,
            "method": method,
            "status_code": 200,
            "response": {"message": "API call successful", "data": {"simulated": True}},
            "timestamp": time.time(),
        }

    async def _simulate_file_operation(
        self, parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Simulate a file system operation."""
        operation = parameters.get("operation", "read")
        file_path = parameters.get("file_path", "/tmp/test.txt")

        await asyncio.sleep(0.05)  # Simulate file I/O delay

        return {
            "operation": f"file_{operation}",
            "file_path": file_path,
            "success": True,
            "size_bytes": 1024,
            "timestamp": time.time(),
        }

    async def _simulate_database_operation(
        self, parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Simulate a database operation."""
        operation = parameters.get("operation", "select")
        table = parameters.get("table", "users")

        await asyncio.sleep(0.15)  # Simulate database query delay

        return {
            "operation": f"db_{operation}",
            "table": table,
            "affected_rows": 1 if operation in ["insert", "update", "delete"] else 5,
            "query_time_ms": 150,
            "timestamp": time.time(),
        }

    async def _simulate_queue_operation(
        self, parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Simulate a message queue operation."""
        operation = parameters.get("operation", "publish")
        queue = parameters.get("queue", "default")

        await asyncio.sleep(0.08)  # Simulate queue operation delay

        return {
            "operation": f"queue_{operation}",
            "queue": queue,
            "message_count": 1,
            "timestamp": time.time(),
        }


class SimpleNodeCanaryOrchestrator:
    """Simplified Canary Orchestrator Node for testing business logic."""

    def __init__(self):
        self._active_workflows = {}
        self._completed_workflows = {}
        self._workflow_count = 0

    async def start_workflow(
        self, input_data: ModelCanaryOrchestratorInput
    ) -> ModelCanaryOrchestratorOutput:
        """Start infrastructure workflow."""
        start_time = time.time()

        try:
            workflow_id = input_data.workflow_id
            operation_type = input_data.operation_type

            if operation_type == "start_workflow":
                workflow_type = input_data.parameters.get(
                    "workflow_type", "infrastructure_startup"
                )
                result = await self._execute_workflow(workflow_id, workflow_type)
            elif operation_type == "stop_workflow":
                result = await self._stop_workflow(workflow_id)
            elif operation_type == "get_status":
                result = self._get_workflow_status(workflow_id)
            else:
                raise ValueError(f"Unsupported operation type: {operation_type}")

            execution_time = int((time.time() - start_time) * 1000)

            return ModelCanaryOrchestratorOutput(
                status="success",
                workflow_result=result,
                execution_metrics={
                    "execution_time_ms": execution_time,
                    "active_workflows": len(self._active_workflows),
                    "completed_workflows": len(self._completed_workflows),
                },
            )

        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ModelCanaryOrchestratorOutput(
                status="failed",
                workflow_result={},
                error_message=str(e),
                execution_metrics={"execution_time_ms": execution_time},
            )

    async def _execute_workflow(
        self, workflow_id: str, workflow_type: str
    ) -> Dict[str, Any]:
        """Execute a workflow based on type."""
        self._active_workflows[workflow_id] = {
            "workflow_id": workflow_id,
            "workflow_type": workflow_type,
            "status": "running",
            "start_time": time.time(),
            "steps_completed": 0,
            "total_steps": 3,
        }

        if workflow_type == "infrastructure_startup":
            return await self._execute_infrastructure_startup(workflow_id)
        elif workflow_type == "infrastructure_shutdown":
            return await self._execute_infrastructure_shutdown(workflow_id)
        elif workflow_type == "canary_deployment":
            return await self._execute_canary_deployment(workflow_id)
        else:
            raise ValueError(f"Unknown workflow type: {workflow_type}")

    async def _execute_infrastructure_startup(self, workflow_id: str) -> Dict[str, Any]:
        """Execute infrastructure startup workflow."""
        steps = [
            ("consul_health_check", 0.1),
            ("vault_initialization", 0.15),
            ("kafka_cluster_verify", 0.12),
        ]

        results = []
        for step_name, delay in steps:
            await asyncio.sleep(delay)
            results.append(f"{step_name}_completed")
            self._active_workflows[workflow_id]["steps_completed"] += 1

        # Move to completed
        workflow = self._active_workflows.pop(workflow_id)
        workflow["status"] = "completed"
        workflow["end_time"] = time.time()
        self._completed_workflows[workflow_id] = workflow
        self._workflow_count += 1

        return {
            "workflow_id": workflow_id,
            "workflow_type": "infrastructure_startup",
            "status": "completed",
            "steps": results,
            "duration_ms": int((workflow["end_time"] - workflow["start_time"]) * 1000),
        }

    async def _execute_infrastructure_shutdown(
        self, workflow_id: str
    ) -> Dict[str, Any]:
        """Execute infrastructure shutdown workflow."""
        steps = [
            ("drain_kafka_queues", 0.2),
            ("vault_token_revoke", 0.05),
            ("consul_deregister", 0.03),
        ]

        results = []
        for step_name, delay in steps:
            await asyncio.sleep(delay)
            results.append(f"{step_name}_completed")
            self._active_workflows[workflow_id]["steps_completed"] += 1

        # Move to completed
        workflow = self._active_workflows.pop(workflow_id)
        workflow["status"] = "completed"
        workflow["end_time"] = time.time()
        self._completed_workflows[workflow_id] = workflow
        self._workflow_count += 1

        return {
            "workflow_id": workflow_id,
            "workflow_type": "infrastructure_shutdown",
            "status": "completed",
            "steps": results,
            "duration_ms": int((workflow["end_time"] - workflow["start_time"]) * 1000),
        }

    async def _execute_canary_deployment(self, workflow_id: str) -> Dict[str, Any]:
        """Execute canary deployment workflow."""
        steps = [
            ("deploy_canary", 0.25),
            ("health_check_canary", 0.1),
            ("traffic_routing", 0.08),
        ]

        results = []
        for step_name, delay in steps:
            await asyncio.sleep(delay)
            results.append(f"{step_name}_completed")
            self._active_workflows[workflow_id]["steps_completed"] += 1

        # Move to completed
        workflow = self._active_workflows.pop(workflow_id)
        workflow["status"] = "completed"
        workflow["end_time"] = time.time()
        self._completed_workflows[workflow_id] = workflow
        self._workflow_count += 1

        return {
            "workflow_id": workflow_id,
            "workflow_type": "canary_deployment",
            "status": "completed",
            "steps": results,
            "duration_ms": int((workflow["end_time"] - workflow["start_time"]) * 1000),
        }

    async def _stop_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """Stop a running workflow."""
        if workflow_id in self._active_workflows:
            workflow = self._active_workflows.pop(workflow_id)
            workflow["status"] = "cancelled"
            workflow["end_time"] = time.time()
            self._completed_workflows[workflow_id] = workflow

            return {
                "workflow_id": workflow_id,
                "status": "cancelled",
                "message": "Workflow stopped successfully",
            }
        else:
            raise ValueError(f"Workflow {workflow_id} not found or not active")

    def _get_workflow_status(self, workflow_id: str) -> Dict[str, Any]:
        """Get workflow status."""
        if workflow_id in self._active_workflows:
            workflow = self._active_workflows[workflow_id]
        elif workflow_id in self._completed_workflows:
            workflow = self._completed_workflows[workflow_id]
        else:
            raise ValueError(f"Workflow {workflow_id} not found")

        return {
            "workflow_id": workflow_id,
            "status": workflow["status"],
            "steps_completed": workflow["steps_completed"],
            "total_steps": workflow.get("total_steps", 0),
            "start_time": workflow["start_time"],
            "end_time": workflow.get("end_time"),
        }


class SimpleNodeCanaryReducer:
    """Simplified Canary Reducer Node for testing business logic."""

    def __init__(self):
        self._aggregation_count = 0
        self._cached_results = {}
        self._infrastructure_status = "unknown"

    async def aggregate_results(
        self, input_data: ModelCanaryReducerInput
    ) -> ModelCanaryReducerOutput:
        """Aggregate results from infrastructure adapters."""
        start_time = time.time()

        try:
            adapter_results = input_data.adapter_results
            operation_type = input_data.operation_type

            if operation_type == "bootstrap":
                result = self._aggregate_bootstrap_results(adapter_results)
            elif operation_type == "health_check":
                result = self._aggregate_health_results(adapter_results)
            elif operation_type == "failover":
                result = self._aggregate_failover_results(adapter_results)
            elif operation_type == "status_aggregation":
                result = self._aggregate_status_results(adapter_results)
            else:
                result = self._aggregate_general_results(adapter_results)

            self._aggregation_count += 1
            self._infrastructure_status = result.get("overall_status", "ready")

            return ModelCanaryReducerOutput(status="success", aggregated_result=result)

        except Exception as e:
            return ModelCanaryReducerOutput(
                status="error", aggregated_result={}, error_message=str(e)
            )

    def _aggregate_bootstrap_results(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate bootstrap initialization results."""
        total_adapters = len(results)
        successful_bootstraps = sum(
            1 for r in results if r.get("bootstrap_successful", False)
        )

        return {
            "operation": "bootstrap",
            "total_adapters": total_adapters,
            "successful_bootstraps": successful_bootstraps,
            "success_rate": (
                successful_bootstraps / total_adapters if total_adapters > 0 else 0
            ),
            "overall_status": (
                "ready" if successful_bootstraps == total_adapters else "degraded"
            ),
            "timestamp": time.time(),
        }

    def _aggregate_health_results(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate health check results."""
        total_adapters = len(results)
        healthy_adapters = sum(1 for r in results if r.get("status") == "healthy")

        health_summary = {}
        for result in results:
            adapter_name = result.get("adapter", "unknown")
            health_summary[adapter_name] = result.get("status", "unknown")

        overall_status = (
            "ready"
            if healthy_adapters == total_adapters
            else "degraded" if healthy_adapters > 0 else "unavailable"
        )

        return {
            "operation": "health_check",
            "total_adapters": total_adapters,
            "healthy_adapters": healthy_adapters,
            "health_summary": health_summary,
            "overall_status": overall_status,
            "timestamp": time.time(),
        }

    def _aggregate_failover_results(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate failover operation results."""
        total_adapters = len(results)
        successful_failovers = sum(
            1 for r in results if r.get("failover_successful", False)
        )

        failover_details = {}
        for result in results:
            adapter_name = result.get("adapter", "unknown")
            failover_details[adapter_name] = {
                "successful": result.get("failover_successful", False),
                "fallback_target": result.get("fallback_target", "none"),
                "failover_time_ms": result.get("failover_time_ms", 0),
            }

        return {
            "operation": "failover",
            "total_adapters": total_adapters,
            "successful_failovers": successful_failovers,
            "failover_details": failover_details,
            "overall_status": "partial" if successful_failovers > 0 else "unavailable",
            "timestamp": time.time(),
        }

    def _aggregate_status_results(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate general status results."""
        total_adapters = len(results)

        status_counts = {}
        adapter_statuses = {}

        for result in results:
            adapter_name = result.get("adapter", "unknown")
            status = result.get("status", "unknown")

            status_counts[status] = status_counts.get(status, 0) + 1
            adapter_statuses[adapter_name] = status

        # Determine overall status based on aggregation rules
        if status_counts.get("healthy", 0) == total_adapters:
            overall_status = "ready"
        elif status_counts.get("healthy", 0) > 0:
            overall_status = "degraded"
        elif status_counts.get("degraded", 0) > 0:
            overall_status = "partial"
        else:
            overall_status = "unavailable"

        return {
            "operation": "status_aggregation",
            "total_adapters": total_adapters,
            "status_counts": status_counts,
            "adapter_statuses": adapter_statuses,
            "overall_status": overall_status,
            "timestamp": time.time(),
        }

    def _aggregate_general_results(
        self, results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate general results when operation type is not specified."""
        return {
            "operation": "general_aggregation",
            "total_results": len(results),
            "results_summary": [f"Result_{i}" for i in range(len(results))],
            "overall_status": "ready",
            "timestamp": time.time(),
        }


class SimpleNodeCanaryGateway:
    """Simplified Canary Gateway Node for testing business logic."""

    def __init__(self):
        self._routing_count = 0
        self._response_cache = {}
        self._routing_metrics = {
            "total_routes": 0,
            "successful_routes": 0,
            "failed_routes": 0,
            "cache_hits": 0,
            "avg_response_time_ms": 0,
        }
        self._response_times = []

    async def route_message(
        self, input_data: ModelGroupGatewayInput
    ) -> ModelGroupGatewayOutput:
        """Route message to target tools and aggregate responses."""
        start_time = time.time()
        cache_key = f"{input_data.operation_type}:{':'.join(input_data.target_tools)}"

        try:
            # Check cache first
            if (
                cache_key in self._response_cache
                and input_data.cache_strategy != "no_cache"
            ):
                self._routing_metrics["cache_hits"] += 1
                cached_response = self._response_cache[cache_key]

                return ModelGroupGatewayOutput(
                    status="success",
                    aggregated_response=cached_response,
                    cache_hit=True,
                    routing_metrics=self._routing_metrics.copy(),
                )

            operation_type = input_data.operation_type
            target_tools = input_data.target_tools
            message_data = input_data.message_data

            if operation_type == "round_robin":
                result = await self._route_round_robin(target_tools, message_data)
            elif operation_type == "broadcast":
                result = await self._route_broadcast(target_tools, message_data)
            elif operation_type == "aggregate":
                result = await self._route_and_aggregate(target_tools, message_data)
            else:
                raise ValueError(f"Unsupported routing operation: {operation_type}")

            # Update metrics
            execution_time = int((time.time() - start_time) * 1000)
            self._response_times.append(execution_time)
            self._routing_count += 1
            self._routing_metrics["total_routes"] += 1
            self._routing_metrics["successful_routes"] += 1
            self._routing_metrics["avg_response_time_ms"] = sum(
                self._response_times
            ) / len(self._response_times)

            # Cache the result
            self._response_cache[cache_key] = result

            return ModelGroupGatewayOutput(
                status="success",
                aggregated_response=result,
                cache_hit=False,
                routing_metrics=self._routing_metrics.copy(),
            )

        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            self._routing_metrics["total_routes"] += 1
            self._routing_metrics["failed_routes"] += 1

            return ModelGroupGatewayOutput(
                status="error",
                aggregated_response={},
                error_message=str(e),
                cache_hit=False,
                routing_metrics=self._routing_metrics.copy(),
            )

    async def _route_round_robin(
        self, target_tools: List[str], message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Route message using round-robin strategy."""
        selected_tool = target_tools[self._routing_count % len(target_tools)]

        # Simulate routing to tool
        await asyncio.sleep(0.1)

        return {
            "routing_strategy": "round_robin",
            "selected_tool": selected_tool,
            "total_targets": len(target_tools),
            "message_processed": True,
            "response": f"Processed by {selected_tool}",
            "timestamp": time.time(),
        }

    async def _route_broadcast(
        self, target_tools: List[str], message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Route message to all tools (broadcast)."""
        responses = []

        for tool in target_tools:
            # Simulate routing to each tool
            await asyncio.sleep(0.05)
            responses.append(
                {
                    "tool": tool,
                    "status": "processed",
                    "response": f"Broadcast processed by {tool}",
                }
            )

        return {
            "routing_strategy": "broadcast",
            "target_tools": target_tools,
            "total_targets": len(target_tools),
            "responses": responses,
            "all_successful": True,
            "timestamp": time.time(),
        }

    async def _route_and_aggregate(
        self, target_tools: List[str], message_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Route message to all tools and aggregate responses."""
        individual_responses = []
        aggregated_data = {"total_responses": 0, "combined_data": {}}

        for tool in target_tools:
            # Simulate routing and response aggregation
            await asyncio.sleep(0.08)
            tool_response = {
                "tool": tool,
                "data": {
                    "processed_count": self._routing_count,
                    "tool_specific": f"data_from_{tool}",
                },
                "status": "success",
            }
            individual_responses.append(tool_response)
            aggregated_data["total_responses"] += 1
            aggregated_data["combined_data"][tool] = tool_response["data"]

        return {
            "routing_strategy": "aggregate",
            "target_tools": target_tools,
            "total_targets": len(target_tools),
            "individual_responses": individual_responses,
            "aggregated_data": aggregated_data,
            "timestamp": time.time(),
        }


# ===== TEST FUNCTIONS =====


async def test_simple_compute_node():
    """Test the simplified Canary Compute node."""
    print("ğŸ§® Testing Simplified COMPUTE Node")
    print("----------------------------------------")

    node = SimpleNodeCanaryCompute()
    test_results = []

    # Test 1: Addition
    try:
        input_data = ModelCanaryComputeInput(
            operation_type="addition",
            data={"values": [1, 2, 3, 4, 5]},
            correlation_id="test-addition-001",
        )
        result = await node.process_computation(input_data)
        success = result.success and result.result.get("result") == 15
        test_results.append(("Addition", success))
        if not success:
            print(f"  âŒ Addition test failed: {result.error_message}")
        else:
            print(
                f"  âœ… Addition: {result.result['values']} = {result.result['result']}"
            )
    except Exception as e:
        test_results.append(("Addition", False))
        print(f"  âŒ Addition test exception: {e}")

    # Test 2: Multiplication
    try:
        input_data = ModelCanaryComputeInput(
            operation_type="multiplication",
            data={"values": [2, 3, 4]},
            correlation_id="test-multiplication-001",
        )
        result = await node.process_computation(input_data)
        success = result.success and result.result.get("result") == 24
        test_results.append(("Multiplication", success))
        if not success:
            print(f"  âŒ Multiplication test failed: {result.error_message}")
        else:
            print(
                f"  âœ… Multiplication: {result.result['values']} = {result.result['result']}"
            )
    except Exception as e:
        test_results.append(("Multiplication", False))
        print(f"  âŒ Multiplication test exception: {e}")

    # Test 3: Aggregation
    try:
        input_data = ModelCanaryComputeInput(
            operation_type="aggregation",
            data={"items": [1, 2, 3, 4, 5, "text", "more_text"]},
            correlation_id="test-aggregation-001",
        )
        result = await node.process_computation(input_data)
        success = result.success and result.result.get("total_items") == 7
        test_results.append(("Aggregation", success))
        if not success:
            print(f"  âŒ Aggregation test failed: {result.error_message}")
        else:
            print(f"  âœ… Aggregation: {result.result['summary']}")
    except Exception as e:
        test_results.append(("Aggregation", False))
        print(f"  âŒ Aggregation test exception: {e}")

    return test_results


async def test_simple_effect_node():
    """Test the simplified Canary Effect node."""
    print("ğŸ¯ Testing Simplified EFFECT Node")
    print("----------------------------------------")

    node = SimpleNodeCanaryEffect()
    test_results = []

    # Test 1: Health Check
    try:
        input_data = ModelCanaryEffectInput(
            operation_type="health_check", correlation_id="test-health-001"
        )
        result = await node.perform_effect(input_data)
        success = result.success and result.operation_result.get("status") == "healthy"
        test_results.append(("Health Check", success))
        if not success:
            print(f"  âŒ Health check failed: {result.error_message}")
        else:
            print(f"  âœ… Health Check: {result.operation_result['status']}")
    except Exception as e:
        test_results.append(("Health Check", False))
        print(f"  âŒ Health check exception: {e}")

    # Test 2: API Call
    try:
        input_data = ModelCanaryEffectInput(
            operation_type="external_api_call",
            parameters={"endpoint": "/users", "method": "GET"},
            correlation_id="test-api-001",
        )
        result = await node.perform_effect(input_data)
        success = result.success and result.operation_result.get("status_code") == 200
        test_results.append(("API Call", success))
        if not success:
            print(f"  âŒ API call failed: {result.error_message}")
        else:
            print(
                f"  âœ… API Call: {result.operation_result['method']} {result.operation_result['endpoint']} -> {result.operation_result['status_code']}"
            )
    except Exception as e:
        test_results.append(("API Call", False))
        print(f"  âŒ API call exception: {e}")

    # Test 3: File Operation
    try:
        input_data = ModelCanaryEffectInput(
            operation_type="file_system_operation",
            parameters={"operation": "write", "file_path": "/tmp/test.txt"},
            correlation_id="test-file-001",
        )
        result = await node.perform_effect(input_data)
        success = result.success and result.operation_result.get("success") == True
        test_results.append(("File Operation", success))
        if not success:
            print(f"  âŒ File operation failed: {result.error_message}")
        else:
            print(
                f"  âœ… File Operation: {result.operation_result['operation']} -> {result.operation_result['file_path']}"
            )
    except Exception as e:
        test_results.append(("File Operation", False))
        print(f"  âŒ File operation exception: {e}")

    return test_results


async def test_simple_orchestrator_node():
    """Test the simplified Canary Orchestrator node."""
    print("ğŸ¼ Testing Simplified ORCHESTRATOR Node")
    print("----------------------------------------")

    node = SimpleNodeCanaryOrchestrator()
    test_results = []

    # Test 1: Infrastructure Startup
    try:
        input_data = ModelCanaryOrchestratorInput(
            operation_type="start_workflow",
            workflow_id="test-startup-001",
            parameters={"workflow_type": "infrastructure_startup"},
            correlation_id="test-orchestrator-001",
        )
        result = await node.start_workflow(input_data)
        success = (
            result.status == "success"
            and result.workflow_result.get("status") == "completed"
        )
        test_results.append(("Infrastructure Startup", success))
        if not success:
            print(f"  âŒ Infrastructure startup failed: {result.error_message}")
        else:
            steps = result.workflow_result.get("steps", [])
            print(
                f"  âœ… Infrastructure Startup: {len(steps)} steps completed in {result.workflow_result.get('duration_ms')}ms"
            )
    except Exception as e:
        test_results.append(("Infrastructure Startup", False))
        print(f"  âŒ Infrastructure startup exception: {e}")

    # Test 2: Canary Deployment
    try:
        input_data = ModelCanaryOrchestratorInput(
            operation_type="start_workflow",
            workflow_id="test-canary-001",
            parameters={"workflow_type": "canary_deployment"},
            correlation_id="test-orchestrator-002",
        )
        result = await node.start_workflow(input_data)
        success = (
            result.status == "success"
            and result.workflow_result.get("status") == "completed"
        )
        test_results.append(("Canary Deployment", success))
        if not success:
            print(f"  âŒ Canary deployment failed: {result.error_message}")
        else:
            steps = result.workflow_result.get("steps", [])
            print(
                f"  âœ… Canary Deployment: {len(steps)} steps completed in {result.workflow_result.get('duration_ms')}ms"
            )
    except Exception as e:
        test_results.append(("Canary Deployment", False))
        print(f"  âŒ Canary deployment exception: {e}")

    # Test 3: Workflow Status
    try:
        # Get status of the completed workflow
        input_data = ModelCanaryOrchestratorInput(
            operation_type="get_status",
            workflow_id="test-startup-001",
            correlation_id="test-orchestrator-003",
        )
        result = await node.start_workflow(input_data)
        success = (
            result.status == "success"
            and result.workflow_result.get("status") == "completed"
        )
        test_results.append(("Workflow Status", success))
        if not success:
            print(f"  âŒ Workflow status failed: {result.error_message}")
        else:
            print(
                f"  âœ… Workflow Status: {result.workflow_result['workflow_id']} -> {result.workflow_result['status']}"
            )
    except Exception as e:
        test_results.append(("Workflow Status", False))
        print(f"  âŒ Workflow status exception: {e}")

    return test_results


async def test_simple_reducer_node():
    """Test the simplified Canary Reducer node."""
    print("ğŸ”„ Testing Simplified REDUCER Node")
    print("----------------------------------------")

    node = SimpleNodeCanaryReducer()
    test_results = []

    # Test 1: Health Check Aggregation
    try:
        adapter_results = [
            {"adapter": "consul", "status": "healthy"},
            {"adapter": "vault", "status": "healthy"},
            {"adapter": "kafka", "status": "degraded"},
        ]
        input_data = ModelCanaryReducerInput(
            adapter_results=adapter_results, operation_type="health_check"
        )
        result = await node.aggregate_results(input_data)
        success = (
            result.status == "success"
            and result.aggregated_result.get("healthy_adapters") == 2
        )
        test_results.append(("Health Aggregation", success))
        if not success:
            print(f"  âŒ Health aggregation failed: {result.error_message}")
        else:
            agg_result = result.aggregated_result
            print(
                f"  âœ… Health Aggregation: {agg_result['healthy_adapters']}/{agg_result['total_adapters']} healthy -> {agg_result['overall_status']}"
            )
    except Exception as e:
        test_results.append(("Health Aggregation", False))
        print(f"  âŒ Health aggregation exception: {e}")

    # Test 2: Bootstrap Aggregation
    try:
        adapter_results = [
            {"adapter": "consul", "bootstrap_successful": True},
            {"adapter": "vault", "bootstrap_successful": True},
            {"adapter": "kafka", "bootstrap_successful": False},
        ]
        input_data = ModelCanaryReducerInput(
            adapter_results=adapter_results, operation_type="bootstrap"
        )
        result = await node.aggregate_results(input_data)
        success = (
            result.status == "success"
            and result.aggregated_result.get("success_rate") == 2 / 3
        )
        test_results.append(("Bootstrap Aggregation", success))
        if not success:
            print(f"  âŒ Bootstrap aggregation failed: {result.error_message}")
        else:
            agg_result = result.aggregated_result
            print(
                f"  âœ… Bootstrap Aggregation: {agg_result['successful_bootstraps']}/{agg_result['total_adapters']} successful -> {agg_result['overall_status']}"
            )
    except Exception as e:
        test_results.append(("Bootstrap Aggregation", False))
        print(f"  âŒ Bootstrap aggregation exception: {e}")

    # Test 3: Status Aggregation
    try:
        adapter_results = [
            {"adapter": "consul", "status": "healthy"},
            {"adapter": "vault", "status": "healthy"},
            {"adapter": "kafka", "status": "healthy"},
        ]
        input_data = ModelCanaryReducerInput(
            adapter_results=adapter_results, operation_type="status_aggregation"
        )
        result = await node.aggregate_results(input_data)
        success = (
            result.status == "success"
            and result.aggregated_result.get("overall_status") == "ready"
        )
        test_results.append(("Status Aggregation", success))
        if not success:
            print(f"  âŒ Status aggregation failed: {result.error_message}")
        else:
            agg_result = result.aggregated_result
            print(
                f"  âœ… Status Aggregation: {agg_result['total_adapters']} adapters -> {agg_result['overall_status']}"
            )
    except Exception as e:
        test_results.append(("Status Aggregation", False))
        print(f"  âŒ Status aggregation exception: {e}")

    return test_results


async def test_simple_gateway_node():
    """Test the simplified Canary Gateway node."""
    print("ğŸšª Testing Simplified GATEWAY Node")
    print("----------------------------------------")

    node = SimpleNodeCanaryGateway()
    test_results = []

    # Test 1: Round Robin Routing
    try:
        input_data = ModelGroupGatewayInput(
            operation_type="round_robin",
            target_tools=["tool_a", "tool_b", "tool_c"],
            message_data={"test": "data"},
            correlation_id="test-gateway-001",
        )
        result = await node.route_message(input_data)
        success = (
            result.status == "success"
            and result.aggregated_response.get("routing_strategy") == "round_robin"
        )
        test_results.append(("Round Robin", success))
        if not success:
            print(f"  âŒ Round robin routing failed: {result.error_message}")
        else:
            selected = result.aggregated_response.get("selected_tool")
            print(
                f"  âœ… Round Robin: Routed to {selected} from {result.aggregated_response.get('total_targets')} targets"
            )
    except Exception as e:
        test_results.append(("Round Robin", False))
        print(f"  âŒ Round robin exception: {e}")

    # Test 2: Broadcast Routing
    try:
        input_data = ModelGroupGatewayInput(
            operation_type="broadcast",
            target_tools=["tool_x", "tool_y"],
            message_data={"broadcast": "message"},
            correlation_id="test-gateway-002",
        )
        result = await node.route_message(input_data)
        success = (
            result.status == "success"
            and len(result.aggregated_response.get("responses", [])) == 2
        )
        test_results.append(("Broadcast", success))
        if not success:
            print(f"  âŒ Broadcast routing failed: {result.error_message}")
        else:
            responses = result.aggregated_response.get("responses", [])
            print(
                f"  âœ… Broadcast: Sent to {len(responses)} targets, all successful: {result.aggregated_response.get('all_successful')}"
            )
    except Exception as e:
        test_results.append(("Broadcast", False))
        print(f"  âŒ Broadcast exception: {e}")

    # Test 3: Aggregate Routing
    try:
        input_data = ModelGroupGatewayInput(
            operation_type="aggregate",
            target_tools=["tool_1", "tool_2", "tool_3"],
            message_data={"aggregate": "data"},
            correlation_id="test-gateway-003",
        )
        result = await node.route_message(input_data)
        success = (
            result.status == "success"
            and result.aggregated_response.get("routing_strategy") == "aggregate"
        )
        test_results.append(("Aggregate", success))
        if not success:
            print(f"  âŒ Aggregate routing failed: {result.error_message}")
        else:
            aggregated = result.aggregated_response.get("aggregated_data", {})
            print(
                f"  âœ… Aggregate: Combined {aggregated.get('total_responses')} responses from {result.aggregated_response.get('total_targets')} targets"
            )
    except Exception as e:
        test_results.append(("Aggregate", False))
        print(f"  âŒ Aggregate exception: {e}")

    # Test 4: Cache Hit
    try:
        # Make the same request again to test caching
        input_data = ModelGroupGatewayInput(
            operation_type="round_robin",
            target_tools=["tool_a", "tool_b", "tool_c"],
            message_data={"test": "data"},
            correlation_id="test-gateway-004",
        )
        result = await node.route_message(input_data)
        success = result.status == "success" and result.cache_hit == True
        test_results.append(("Cache Hit", success))
        if not success:
            print(f"  âŒ Cache test failed: {result.error_message}")
        else:
            print(
                f"  âœ… Cache Hit: Response served from cache, cache hits: {result.routing_metrics.get('cache_hits')}"
            )
    except Exception as e:
        test_results.append(("Cache Hit", False))
        print(f"  âŒ Cache test exception: {e}")

    return test_results


async def main():
    """Main test runner."""
    print("ğŸ§ª Testing Simplified Canary Nodes Functionality")
    print("==================================================")
    print()

    all_results = []

    # Test all nodes
    test_functions = [
        ("COMPUTE", test_simple_compute_node),
        ("EFFECT", test_simple_effect_node),
        ("ORCHESTRATOR", test_simple_orchestrator_node),
        ("REDUCER", test_simple_reducer_node),
        ("GATEWAY", test_simple_gateway_node),
    ]

    for node_type, test_func in test_functions:
        try:
            results = await test_func()
            all_results.extend(results)
            print()
        except Exception as e:
            print(f"âŒ {node_type} node test failed with exception: {e}")
            all_results.append((f"{node_type} - Exception", False))
            print()

    # Summary
    print("==================================================")
    print("ğŸ§ª SIMPLIFIED CANARY NODES TEST SUMMARY")
    print("==================================================")

    total_tests = len(all_results)
    passed_tests = sum(1 for _, success in all_results if success)
    failed_tests = total_tests - passed_tests
    success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

    print(f"Tests Run: {total_tests}")
    print(f"Tests Passed: {passed_tests}")
    print(f"Tests Failed: {failed_tests}")
    print(f"Success Rate: {success_rate:.1f}%")
    print()

    if failed_tests == 0:
        print("ğŸ‰ All tests passed!")
        print("âœ… Canary node business logic is functioning correctly.")
    else:
        print(f"âš ï¸  {failed_tests} tests failed.")
        print("âŒ Please review the failed tests before deploying.")

    print()
    print("ğŸ“Š NODE-BY-NODE RESULTS:")
    current_node = None
    for test_name, success in all_results:
        if " - " in test_name:
            node_name = test_name.split(" - ")[0]
            if node_name != current_node:
                current_node = node_name
                print(f"\n{current_node}:")
        status = "âœ…" if success else "âŒ"
        print(f"  {status} {test_name}")

    return failed_tests == 0


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
