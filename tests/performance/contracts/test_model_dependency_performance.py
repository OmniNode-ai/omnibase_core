"""
Performance Benchmarks for ModelDependency Validation Operations.

Tests regex pattern performance, security validation, and large-scale
dependency resolution to ensure ONEX compliance under load.

ZERO TOLERANCE: Validation must remain fast even with complex patterns.
"""

import time

import pytest

from omnibase_core.models.contracts.model_dependency import (
    EnumDependencyType,
    ModelDependency,
)
from omnibase_core.models.errors.model_onex_error import ModelOnexError
from omnibase_core.models.primitives.model_semver import ModelSemVer


@pytest.mark.performance
@pytest.mark.timeout(60)
class TestModelDependencyPerformance:
    """Performance benchmarks for dependency validation operations."""

    @pytest.mark.performance
    @pytest.mark.skip(
        reason="Pattern _CAMEL_TO_SNAKE_PATTERN removed in PR to reduce memory footprint"
    )
    def test_regex_compilation_performance(self):
        """Test that pre-compiled regex patterns perform well."""
        # Access class-level compiled patterns
        module_pattern = ModelDependency._MODULE_PATTERN
        camel_to_snake_pattern = ModelDependency._CAMEL_TO_SNAKE_PATTERN

        # Test module paths for regex performance
        test_modules = [
            "simple.module",
            "very.long.module.path.with.many.segments.that.could.slow.down.regex",
            "a" * 50 + ".module",  # Long segment names
            "omnibase_core.models.workflow.dependency_manager",
            "service-name.sub-module.component_handler",
        ]

        # Benchmark module pattern matching
        start_time = time.perf_counter()
        iterations = 10000

        for _ in range(iterations):
            for module in test_modules:
                match = module_pattern.match(module)
                assert match is not None, f"Valid module should match: {module}"

        module_time = time.perf_counter() - start_time
        avg_module_time_us = (module_time / (iterations * len(test_modules))) * 1000000

        # Benchmark camel-to-snake conversion
        camel_cases = [
            "SimpleClass",
            "VeryLongClassNameWithManyWordsToTest",
            "APIHandler",
            "XMLParser",
            "HTTPSConnection",
        ]

        start_time = time.perf_counter()

        for _ in range(iterations):
            for camel in camel_cases:
                converted = camel_to_snake_pattern.sub("_", camel).lower()
                # Just perform the conversion for timing - don't assert results

        snake_time = time.perf_counter() - start_time
        avg_snake_time_us = (snake_time / (iterations * len(camel_cases))) * 1000000

        # Performance targets: <50 microseconds per operation
        assert avg_module_time_us < 50, (
            f"Module regex too slow: {avg_module_time_us:.2f}Î¼s"
        )
        assert avg_snake_time_us < 50, (
            f"Snake case regex too slow: {avg_snake_time_us:.2f}Î¼s"
        )

        print(f"âœ… Module regex performance: {avg_module_time_us:.2f}Î¼s per match")
        print(f"âœ… Snake case conversion: {avg_snake_time_us:.2f}Î¼s per conversion")

    @pytest.mark.performance
    def test_security_validation_performance(self):
        """Test performance of security validation checks."""
        # Security violation patterns (should be fast to reject)
        malicious_modules = [
            "../etc/passwd",
            "module/with/slashes",
            "module\\with\\backslashes",
            "mod<script>ule",
            "mod|pipe|ule",
            "mod;semicolon;ule",
            "mod`backtick`ule",
            "mod$dollar$ule",
            "." + "very_long_module_name" * 20,  # >200 chars
        ]

        # Test security validation performance
        start_time = time.perf_counter()
        rejected_count = 0

        for _ in range(1000):
            for malicious in malicious_modules:
                try:
                    ModelDependency(name="TestDep", module=malicious)
                except (ModelOnexError, ValueError) as e:
                    if any(
                        violation in str(e)
                        for violation in [
                            "security",
                            "traversal",
                            "injection",
                            "excessive_length",
                        ]
                    ):
                        rejected_count += 1

        security_time = time.perf_counter() - start_time
        # Total operations: 1000 iterations x 9 malicious patterns = 9000 operations
        # Convert to milliseconds: (total_seconds / total_operations) * 1000
        total_security_operations = len(malicious_modules) * 1000
        avg_security_time_ms = (security_time / total_security_operations) * 1000

        # Performance targets
        # Note: Threshold increased to 3.0s to account for parallel test execution variance:
        # - 9000 operations (1000 iterations x 9 malicious patterns)
        # - Baseline: ~1.05-1.26s (0.14ms per operation)
        # - Each validation performs: path traversal checks, shell injection detection,
        #   privileged keyword scanning, and creates full ModelOnexError with context
        # - 3.0s accommodates:
        #   * Parallel test execution load (16 splits x 4-12 workers per split)
        #   * CI/CD environment CPU contention and variance
        #   * System load from concurrent pytest-xdist workers
        # - Still catches actual regressions (>3.0s = >333Î¼s per operation vs 140Î¼s baseline)
        # - Per-operation threshold: 0.25ms (updated for parallel execution variance)
        assert security_time < 3.0, (
            f"Security validation too slow: {security_time:.2f}s (threshold: 3.0s for parallel execution)"
        )
        assert avg_security_time_ms < 0.25, (
            f"Average security check too slow: {avg_security_time_ms:.3f}ms (threshold: 0.25ms for parallel execution)"
        )
        assert rejected_count >= 7000, (
            f"Expected â‰¥7000 security rejections, got {rejected_count}"
        )

        print(f"âœ… Security validation: {avg_security_time_ms:.3f}ms per check")
        print(
            f"âœ… Security rejections: {rejected_count} of {len(malicious_modules) * 1000}"
        )

    @pytest.mark.performance
    def test_dependency_creation_performance(self):
        """Test performance of dependency object creation and validation."""
        # Test data for dependency creation
        test_cases = [
            {"name": "SimpleProtocol", "dependency_type": EnumDependencyType.PROTOCOL},
            {
                "name": "ServiceHandler",
                "module": "service.handler",
                "dependency_type": EnumDependencyType.SERVICE,
            },
            {
                "name": "ExternalLib",
                "module": "external.library",
                "dependency_type": EnumDependencyType.EXTERNAL,
            },
            {
                "name": "ComplexDep",
                "module": "complex.module.path",
                "dependency_type": EnumDependencyType.MODULE,
                "version": ModelSemVer(major=2, minor=1, patch=0),
                "required": False,
                "description": "Complex dependency with all fields",
            },
        ]

        # Benchmark dependency creation
        start_time = time.perf_counter()
        created_count = 0

        for _ in range(2500):  # 2500 iterations Ã— 4 cases = 10,000 total
            for case in test_cases:
                try:
                    dep = ModelDependency(**case)
                    assert dep.name == case["name"]
                    created_count += 1
                except Exception:
                    pass  # Shouldn't happen with valid test cases

        creation_time = time.perf_counter() - start_time
        avg_creation_time_ms = (creation_time / created_count) * 1000

        # Performance targets
        assert creation_time < 2.0, (
            f"Dependency creation too slow: {creation_time:.2f}s"
        )
        assert avg_creation_time_ms < 0.5, (
            f"Average creation too slow: {avg_creation_time_ms:.3f}ms"
        )
        assert created_count == 10000, f"Expected 10000 creations, got {created_count}"

        print(f"âœ… Dependency creation: {avg_creation_time_ms:.3f}ms per dependency")
        print(
            f"âœ… Total creation time: {creation_time:.2f}s for {created_count} dependencies"
        )

    @pytest.mark.performance
    def test_onex_pattern_validation_performance(self):
        """Test performance of ONEX naming pattern validation."""
        # Protocol dependencies (should match patterns)
        protocol_deps = [
            {"name": "ProtocolEventBus", "module": "protocol.event_bus"},
            {"name": "ProtocolHealthCheck", "module": "omnibase.protocol.health"},
            {"name": "EventProtocol", "module": "events.protocol_handler"},
        ]

        # Non-protocol dependencies (flexible patterns)
        other_deps = [
            {
                "name": "ServiceManager",
                "module": "service.manager",
                "dependency_type": EnumDependencyType.SERVICE,
            },
            {
                "name": "UtilModule",
                "module": "utils.helper",
                "dependency_type": EnumDependencyType.MODULE,
            },
            {
                "name": "ExternalAPI",
                "module": "external.api",
                "dependency_type": EnumDependencyType.EXTERNAL,
            },
        ]

        # Benchmark pattern validation
        start_time = time.perf_counter()
        pattern_matches = 0

        for _ in range(1000):
            # Test protocol pattern matching
            for proto in protocol_deps:
                dep = ModelDependency(
                    name=proto["name"],
                    module=proto["module"],
                    dependency_type=EnumDependencyType.PROTOCOL,
                )
                if dep.matches_onex_patterns():
                    pattern_matches += 1

            # Test other pattern matching
            for other in other_deps:
                dep = ModelDependency(**other)
                if dep.matches_onex_patterns():
                    pattern_matches += 1

        pattern_time = time.perf_counter() - start_time
        # Total operations: 1000 iterations x 6 dependencies = 6000 operations
        # Convert to milliseconds: (total_seconds / total_operations) * 1000
        total_pattern_operations = (len(protocol_deps) + len(other_deps)) * 1000
        avg_pattern_time_ms = (pattern_time / total_pattern_operations) * 1000

        # Performance targets
        assert pattern_time < 1.0, f"Pattern validation too slow: {pattern_time:.2f}s"
        assert avg_pattern_time_ms < 0.1, (
            f"Average pattern check too slow: {avg_pattern_time_ms:.3f}ms"
        )
        assert pattern_matches >= 5000, (
            f"Expected â‰¥5000 pattern matches, got {pattern_matches}"
        )

        print(f"âœ… ONEX pattern validation: {avg_pattern_time_ms:.3f}ms per check")
        print(f"âœ… Pattern matches: {pattern_matches} of {6000} total checks")

    @pytest.mark.performance
    def test_large_dependency_set_performance(self):
        """Test performance with large sets of dependencies."""
        num_dependencies = 5000

        # Generate diverse dependency data
        dependency_data = []
        for i in range(num_dependencies):
            dep_type = [
                EnumDependencyType.PROTOCOL,
                EnumDependencyType.SERVICE,
                EnumDependencyType.MODULE,
                EnumDependencyType.EXTERNAL,
            ][i % 4]

            name = f"Dependency{i:04d}"
            module = f"module.group{i % 100}.component{i % 10}"

            # Add complexity for some dependencies
            version = (
                ModelSemVer(major=1, minor=i % 10, patch=i % 5) if i % 7 == 0 else None
            )
            required = i % 3 != 0
            description = f"Generated dependency {i}" if i % 5 == 0 else None

            dependency_data.append(
                {
                    "name": name,
                    "module": module,
                    "dependency_type": dep_type,
                    "version": version,
                    "required": required,
                    "description": description,
                }
            )

        # Benchmark large dependency set creation
        start_time = time.perf_counter()
        dependencies: list[ModelDependency] = []

        for data in dependency_data:
            dep = ModelDependency(**data)
            dependencies.append(dep)

        creation_time = time.perf_counter() - start_time

        # Benchmark dependency analysis
        start_time = time.perf_counter()

        protocol_count = sum(1 for d in dependencies if d.is_protocol())
        service_count = sum(1 for d in dependencies if d.is_service())
        external_count = sum(1 for d in dependencies if d.is_external())
        required_count = sum(1 for d in dependencies if d.required)
        versioned_count = sum(1 for d in dependencies if d.version is not None)

        analysis_time = time.perf_counter() - start_time

        # Performance targets
        assert creation_time < 3.0, f"Large set creation too slow: {creation_time:.2f}s"
        assert analysis_time < 0.1, f"Large set analysis too slow: {analysis_time:.2f}s"

        # Verify correctness
        assert len(dependencies) == num_dependencies
        assert protocol_count > 0 and service_count > 0 and external_count > 0
        assert required_count > 0 and versioned_count > 0

        throughput = num_dependencies / creation_time
        print(
            f"âœ… Large set creation: {creation_time:.2f}s for {num_dependencies} dependencies"
        )
        print(f"âœ… Creation throughput: {throughput:.0f} dependencies/sec")
        print(f"âœ… Analysis time: {analysis_time * 1000:.2f}ms")
        print(
            f"ðŸ“Š Types: {protocol_count} protocol, {service_count} service, {external_count} external"
        )

    @pytest.mark.performance
    def test_memory_efficiency(self):
        """Test memory efficiency of dependency objects."""
        import sys
        import tracemalloc

        # Start memory tracking
        tracemalloc.start()
        snapshot1 = tracemalloc.take_snapshot()

        # Create dependencies with various configurations
        dependencies = []
        for i in range(1000):
            # Vary complexity to test different memory patterns
            if i % 4 == 0:
                # Simple dependency
                dep = ModelDependency(name=f"Simple{i}")
            elif i % 4 == 1:
                # With module
                dep = ModelDependency(name=f"Module{i}", module=f"module.path{i}")
            elif i % 4 == 2:
                # With version
                dep = ModelDependency(
                    name=f"Versioned{i}",
                    version=ModelSemVer(major=1, minor=0, patch=i % 10),
                )
            else:
                # Full dependency
                dep = ModelDependency(
                    name=f"Full{i}",
                    module=f"full.module.path{i}",
                    dependency_type=EnumDependencyType.SERVICE,
                    version=ModelSemVer(major=2, minor=1, patch=0),
                    required=True,
                    description=f"Full dependency {i}",
                )

            dependencies.append(dep)

        snapshot2 = tracemalloc.take_snapshot()

        # Calculate memory usage
        top_stats = snapshot2.compare_to(snapshot1, "lineno")
        total_memory_kb = sum(stat.size for stat in top_stats) / 1024
        memory_per_dep_bytes = (total_memory_kb * 1024) / len(dependencies)

        # Memory efficiency targets
        # Note: Adjusted threshold to 1050KB to account for acceptable variance in memory measurements
        # across different Python versions and environments (original 1000KB was 0.7% under actual usage)
        assert memory_per_dep_bytes < 2000, (
            f"Memory per dependency too high: {memory_per_dep_bytes:.1f} bytes"
        )
        assert total_memory_kb < 1050, (
            f"Total memory usage too high: {total_memory_kb:.1f} KB"
        )

        print(f"âœ… Memory per dependency: {memory_per_dep_bytes:.1f} bytes")
        print(
            f"âœ… Total memory usage: {total_memory_kb:.1f} KB for {len(dependencies)} dependencies"
        )

        # Test object size
        simple_dep = ModelDependency(name="Test")
        complex_dep = ModelDependency(
            name="ComplexTest",
            module="complex.module.path",
            dependency_type=EnumDependencyType.PROTOCOL,
            version=ModelSemVer(major=1, minor=0, patch=0),
            required=True,
            description="Complex dependency for testing",
        )

        simple_size = sys.getsizeof(simple_dep)
        complex_size = sys.getsizeof(complex_dep)

        print(f"âœ… Simple dependency size: {simple_size} bytes")
        print(f"âœ… Complex dependency size: {complex_size} bytes")

        tracemalloc.stop()

    @pytest.mark.performance
    @pytest.mark.slow
    def test_stress_validation_performance(self):
        """Stress test validation under high load."""
        import queue
        import threading

        num_threads = 20
        deps_per_thread = 500
        results: queue.Queue[tuple[int, int, int, float]] = queue.Queue()

        def stress_validation(thread_id: int) -> None:
            """Perform stress validation in a thread."""
            start_time = time.perf_counter()
            successes = 0
            failures = 0

            for i in range(deps_per_thread):
                try:
                    # Mix of valid and invalid dependencies
                    if i % 10 == 0:
                        # Invalid module path (should fail)
                        ModelDependency(name=f"Invalid{i}", module="../invalid/path")
                        failures += 1  # Should not reach here
                    else:
                        # Valid dependency
                        dep = ModelDependency(
                            name=f"Valid{thread_id}_{i}",
                            module=f"thread{thread_id}.module{i}",
                            dependency_type=EnumDependencyType.MODULE,
                            required=i % 2 == 0,
                        )
                        successes += 1
                except Exception:
                    # Expected for invalid cases
                    failures += 1

            end_time = time.perf_counter()
            results.put((thread_id, successes, failures, end_time - start_time))

        # Run stress test
        threads = []
        overall_start = time.perf_counter()

        for i in range(num_threads):
            thread = threading.Thread(target=stress_validation, args=(i,))
            thread.start()
            threads.append(thread)

        for thread in threads:
            thread.join()

        overall_time = time.perf_counter() - overall_start

        # Collect results
        total_successes = 0
        total_failures = 0
        max_thread_time: float = 0.0

        while not results.empty():
            _, successes, failures, thread_time = results.get()
            total_successes += successes
            total_failures += failures
            max_thread_time = max(max_thread_time, thread_time)

        total_ops = total_successes + total_failures
        throughput = total_ops / overall_time

        # Performance targets for stress test
        assert overall_time < 10.0, f"Stress test too slow: {overall_time:.2f}s"
        assert throughput > 1000, f"Stress throughput too low: {throughput:.1f} ops/sec"
        assert total_successes > total_failures, (
            f"Too many failures: {total_failures}/{total_ops}"
        )

        print(f"âœ… Stress test: {throughput:.1f} ops/sec throughput")
        print(
            f"âœ… {total_successes} successes, {total_failures} failures in {overall_time:.2f}s"
        )
        print(f"âœ… Max thread time: {max_thread_time:.2f}s")


if __name__ == "__main__":
    # Run performance benchmarks
    pytest.main([__file__, "-v", "-s"])
