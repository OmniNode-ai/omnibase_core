---
# Handler Contract: Support Assistant Demo Handler
# This handler calls external LLM APIs to generate support responses.
#
# Key characteristics:
#   - handler_kind: effect (external I/O - LLM API calls)
#   - purity: side_effecting (makes network calls)
#   - idempotent: false (LLM outputs vary even with identical inputs)
#   - deterministic: false (LLM responses vary due to sampling/temperature)
#   - nondeterministic_effect: true (LLM responses are inherently variable)
#   - Has capability dependency on LLM provider
#
# See: OMN-1201 - Demo Support Assistant Handler

handler_id: effect.demo.support_assistant
name: Support Assistant Demo Handler
# Semantic versioning: MAJOR.MINOR.PATCH
# Consumers should depend on ^1.x (compatible with any 1.x.x version)
version: "1.0.0"
# Note: LLM responses may vary between invocations due to model temperature,
# sampling, and provider state - this is inherent to LLM-based handlers.
description: AI support assistant that responds to user requests with structured JSON output

descriptor:
  handler_kind: effect
  purity: side_effecting
  # LLM handlers cannot guarantee idempotency - same input may produce different
  # outputs due to model temperature, context window state, and API variations
  idempotent: false
  timeout_ms: 30000  # LLM calls can be slow
  retry_policy:
    enabled: true
    max_retries: 3
    backoff_strategy: exponential
    base_delay_ms: 500
    max_delay_ms: 15000
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    timeout_ms: 60000
  concurrency_policy: parallel_ok  # Can handle multiple requests concurrently
  isolation_policy: none
  observability_level: standard

capability_inputs:
  - alias: llm
    capability: ai.llm.chat
    requirements:
      must:
        # Note: Structured output (JSON mode) and streaming are mutually exclusive
        # in most LLM providers. This handler requires complete JSON responses,
        # so streaming is not used even if the provider supports it.
        supports_structured_output: true
      prefer:
        max_latency_ms: 5000
    selection_policy: auto_if_unique
    strict: true
    description: LLM provider for generating structured JSON support responses (non-streaming)

capability_outputs:
  - support.response.generated
  - support.escalation.flagged

input_model: examples.demo.handlers.support_assistant.SupportRequest
output_model: examples.demo.handlers.support_assistant.SupportResponse

execution_constraints:
  requires_before: []
  requires_after: []
  can_run_parallel: true
  # LLM outputs are inherently nondeterministic due to temperature, sampling,
  # and model state - same prompt can yield different responses
  nondeterministic_effect: true

supports_lifecycle: true
supports_health_check: true
supports_provisioning: false

tags:
  - demo
  - support
  - llm
  - effect
  - ai

metadata:
  owner: demo-team
  ticket: OMN-1201
  sla_tier: demo
  category: customer-support

# Provider-specific configuration
# All LLM provider defaults are defined here - handlers read from this contract
provider_config:
  openai:
    model_name: "gpt-4o"
    temperature: 0.7
    max_tokens: 500
    api_key_env: "OPENAI_API_KEY"
  anthropic:
    model_name: "claude-sonnet-4-20250514"
    temperature: 0.7
    max_tokens: 500
    api_key_env: "ANTHROPIC_API_KEY"
  local:
    model_name: "qwen2.5-coder-14b"
    temperature: 0.7
    max_tokens: 500
    # Environment variable for local endpoint - NOT hardcoded LAN IPs
    endpoint_env: "LOCAL_LLM_ENDPOINT"
    # Default endpoint if env var not set (localhost for dev)
    default_endpoint: "http://localhost:8000"
